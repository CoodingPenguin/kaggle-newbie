{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras + TF LB 0.18 by wvadim - Third Try\n",
    "* Data analysis - [Exploration & Transforming Images in Python](https://www.kaggle.com/muonneutrino/exploration-transforming-images-in-python)\n",
    "* Image conversion, Network architecture - [Keras Model for Beginners (0.210 on LB)+EDA+R&D](https://www.kaggle.com/tivigovidiu/keras-model-for-beginners-0-210-on-lb-eda-r-d)\n",
    "* Some ideas - [A Keras Prototype (0.21174 on PL)](https://www.kaggle.com/knowledgegrappler/a-keras-prototype-0-21174-on-pl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-09-05T18:27:02.068Z"
    }
   },
   "outputs": [],
   "source": [
    "# seed값 설정\n",
    "import numpy as np\n",
    "np.random.seed(98643)\n",
    "import tensorflow as tf\n",
    "tf.set_random_seed(663)\n",
    "\n",
    "# 텐서플로우 경고 숨기기\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-09-05T18:27:02.072Z"
    }
   },
   "outputs": [],
   "source": [
    "# 이미지조작\n",
    "from skimage.restoration import (denoise_tv_chambolle, denoise_bilateral,\n",
    "                                denoise_wavelet, estimate_sigma,\n",
    "                                denoise_tv_bregman, denoise_nl_means)\n",
    "from skimage.filters import gaussian\n",
    "from skimage.color import rgb2gray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-09-05T18:27:02.075Z"
    }
   },
   "outputs": [],
   "source": [
    "# 데이터 로드 및 시각화\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-09-05T18:27:02.078Z"
    }
   },
   "outputs": [],
   "source": [
    "# 데이터 학습\n",
    "from keras.layers import Conv2D, MaxPooling2D, Dense, Dropout, Input, Flatten, GlobalAveragePooling2D, Lambda\n",
    "from keras.layers import GlobalMaxPooling2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.merge import Concatenate\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint, Callback, EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Define Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-09-05T18:27:02.083Z"
    }
   },
   "outputs": [],
   "source": [
    "# RGB 배열 생성\n",
    "def color_composite(data):\n",
    "    rgb_arrays = []\n",
    "    for i, row in data.iterrows():\n",
    "        band_1 = np.array(row['band_1']).reshape(75, 75)\n",
    "        band_2 = np.array(row['band_2']).reshape(75, 75)\n",
    "        band_3 = band_1 / band_2\n",
    "        \n",
    "        # rgb 크기 0-1사이의 값으로 변환\n",
    "        r = (band_1 + abs(band_1.min())) / np.max((band_1 + abs(band_1.min())))\n",
    "        g = (band_2 + abs(band_2.min())) / np.max((band_2 + abs(band_2.min())))\n",
    "        b = (band_3 + abs(band_3.min())) / np.max((band_3 + abs(band_3.min())))\n",
    "        \n",
    "        rgb = np.dstack((r, g, b))\n",
    "        rgb_arrays.append(rgb)\n",
    "        \n",
    "    return np.array(rgb_arrays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-09-05T18:27:02.088Z"
    }
   },
   "outputs": [],
   "source": [
    "# 노이즈 제거\n",
    "def denoise(X, weight, multichannel):\n",
    "    return np.asarray([denoise_tv_chambolle(item, weight=weight, multichannel=multichannel) for  item in X])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-09-05T18:27:02.092Z"
    }
   },
   "outputs": [],
   "source": [
    "# 가우시안 필터링\n",
    "def smooth(X, sigma):\n",
    "    return np.asarray([gaussian(item, sigma=sigma) for item in X])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-09-05T18:27:02.096Z"
    }
   },
   "outputs": [],
   "source": [
    "# 회색조 이미지로 변환\n",
    "def grayscale(X):\n",
    "    return np.asarray([rgb2gray(item) for item in X])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Fill in missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-09-05T18:27:02.101Z"
    }
   },
   "outputs": [],
   "source": [
    "# 학습 데이터 로드\n",
    "train = pd.read_json('../data/train.json')\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-09-05T18:27:02.106Z"
    }
   },
   "outputs": [],
   "source": [
    "# 결측치 개수\n",
    "print('band_1 :', len(train.loc[train['band_1'] == 'na', 'band_1']))\n",
    "print('band_2 :', len(train.loc[train['band_2'] == 'na', 'band_2']))\n",
    "print('inc_angle :', len(train.loc[train['inc_angle'] == 'na', 'inc_angle']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-09-05T18:27:02.111Z"
    }
   },
   "outputs": [],
   "source": [
    "# inc_angle의 결측치 채우기\n",
    "train.inc_angle = train.inc_angle.replace('na', 0)\n",
    "train.inc_angle = train.inc_angle.astype(float).fillna(0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Final Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-09-05T18:27:02.116Z"
    }
   },
   "outputs": [],
   "source": [
    "# 플래그 변수 정의\n",
    "train_all = True\n",
    "train_b = True or train_all\n",
    "train_img = True or train_all\n",
    "train_total = True or train_all\n",
    "predict_submission = True or train_all\n",
    "\n",
    "clean_all = False\n",
    "clean_b = False or clean_all\n",
    "clean_img = False or clean_all\n",
    "\n",
    "load_all = False\n",
    "load_b = False or load_all\n",
    "load_img = False or load_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-09-05T18:27:02.122Z"
    }
   },
   "outputs": [],
   "source": [
    "# 데이터셋 전처리 및 데이터셋 생성\n",
    "def create_dataset(frame, labeled, smooth_rgb=0.2, smooth_gray=0.5, weight_rgb=0.05, weight_gray=0.05):\n",
    "    band_1, band_2, images = frame['band_1'].values, frame['band_2'].values, color_composite(frame)\n",
    "    \n",
    "    # 데이터프레임을 np.array로 변환\n",
    "    to_arr = lambda x: np.asarray([np.asarray(item) for item in x])\n",
    "    band_1 = to_arr(band_1)\n",
    "    band_2 = to_arr(band_2)\n",
    "    band_3 = (band_1 + band_2) / 2\n",
    "    \n",
    "    # 벡터에서 이미지 형태로 변환\n",
    "    gray_reshape = lambda x:np.asarray([item.reshape(75, 75) for item in x])\n",
    "    band_1 = gray_reshape(band_1)\n",
    "    band_2 = gray_reshape(band_2)\n",
    "    band_3 = gray_reshape(band_3)\n",
    "    \n",
    "    # 노이즈 제거 및 가우시안 필터링\n",
    "    print('Denoising and reshaping...')\n",
    "    \n",
    "    # 1. bandwidth 데이터\n",
    "    if train_b and clean_b:\n",
    "        band_1 = smooth(denoise(band_1, weight_gray, False), smooth_gray)\n",
    "        print('Gray 1 done')\n",
    "        \n",
    "        band_2 = smooth(denoise(band_2, weight_gray, False), smooth_gray)\n",
    "        print('Gray 2 done')\n",
    "        \n",
    "        band_3 = smooth(denoise(band_3, weight_gray, False), smooth_gray)\n",
    "        print('Gray 3 done')\n",
    "        \n",
    "    # 2. 이미지 데이터\n",
    "    if train_img and clean_img:\n",
    "        images = smooth(denoise(images, weight_rgb, True), smooth_gray)\n",
    "        print('RGB done')\n",
    "        \n",
    "    # 모델에 넣기 적합한 형태로 변환\n",
    "    tf_reshape = lambda x: np.asarray([item.reshape(75, 75, 1) for item in x])\n",
    "    band_1 = tf_reshape(band_1)\n",
    "    band_2 = tf_reshape(band_2)\n",
    "    band_3 = tf_reshape(band_3)\n",
    "    band = np.concatenate([band_1, band_2, band_3], axis=3)\n",
    "    \n",
    "    # 라벨 추출\n",
    "    if labeled:\n",
    "        y = np.array(frame['is_iceberg'])\n",
    "    else:\n",
    "        y = None\n",
    "    \n",
    "    return y, band, images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-09-05T18:27:02.127Z"
    }
   },
   "outputs": [],
   "source": [
    "# 데이터셋 생성\n",
    "y_train, X_b, X_images = create_dataset(train, True)\n",
    "\n",
    "# X_b와 X_images 비교\n",
    "print('X_b[0][0][0] :', X_b[0][0][0])\n",
    "print('X_images[0][0][0] :', X_images[0][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-09-05T18:27:02.155Z"
    }
   },
   "outputs": [],
   "source": [
    "# 9개의 데이터를 임의로 추출해 시각화\n",
    "fig = plt.figure(200, figsize=(15, 15))\n",
    "random_idx = np.random.choice(range(len(X_images)), 9, False)\n",
    "subset = X_images[random_idx]\n",
    "for i in range(9):\n",
    "    ax = fig.add_subplot(3, 3, i+1)\n",
    "    ax.imshow(subset[i])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4. Compare Original and preprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-09-05T18:27:02.160Z"
    }
   },
   "outputs": [],
   "source": [
    "# 노이즈 제거 및 필터링 전 band_1 데이터\n",
    "fig = plt.figure(202, figsize=(15, 15))\n",
    "band_1_x = train['band_1'].values\n",
    "subset = np.asarray(band_1_x)[random_idx]\n",
    "subset = np.asarray([np.asarray(item).reshape(75, 75) for item in subset])\n",
    "for i in range(9):\n",
    "    ax = fig.add_subplot(3, 3, i+1)\n",
    "    ax.imshow(subset[i])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-09-05T18:27:02.181Z"
    }
   },
   "outputs": [],
   "source": [
    "# 노이즈 제거 후 band_1 데이터\n",
    "fig = plt.figure(202, figsize=(15, 15))\n",
    "band_1_x = train['band_1'].values\n",
    "subset = np.asarray(band_1_x)[random_idx]\n",
    "subset = denoise(np.asarray([np.asarray(item).reshape(75, 75) for item in subset]), 0.05, False)\n",
    "for i in range(9):\n",
    "    ax = fig.add_subplot(3, 3, i+1)\n",
    "    ax.imshow(subset[i])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-09-05T18:27:02.186Z"
    }
   },
   "outputs": [],
   "source": [
    "# 노이즈 제거 및 가우시언 필터링 후 band_1 데이터\n",
    "fig = plt.figure(202, figsize=(15, 15))\n",
    "subset = np.asarray(band_1_x)[random_idx]\n",
    "subset = smooth(denoise(np.asarray([np.asarray(item).reshape(75, 75) for item in subset]), 0.05, False), 0.5)\n",
    "for i in range(9):\n",
    "    ax = fig.add_subplot(3, 3, i+1)\n",
    "    ax.imshow(subset[i])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-09-05T18:27:02.204Z"
    }
   },
   "outputs": [],
   "source": [
    "# bandwidth와 이미지 데이터셋으로 학습하는 기본 모델 정의\n",
    "def get_model_notebook(lr, decay, channels, relu_type='relu'):\n",
    "    # 입력 데이터\n",
    "    input_1 = Input(shape=(75, 75, channels))\n",
    "    \n",
    "    ################# 모델 설계 #################\n",
    "    # Conv Layer 1\n",
    "    fcnn = Conv2D(32, kernel_size=(3, 3), activation=relu_type)(BatchNormalization()(input_1))\n",
    "    fcnn = MaxPooling2D((3, 3))(fcnn)\n",
    "    fcnn = Dropout(0.2)(fcnn)\n",
    "    \n",
    "    # Conv Layer 2\n",
    "    fcnn = Conv2D(64, kernel_size=(3, 3), activation=relu_type)(fcnn)\n",
    "    fcnn = MaxPooling2D((2, 2), strides=(2, 2))(fcnn)\n",
    "    fcnn = Dropout(0.2)(fcnn)\n",
    "    \n",
    "    # Conv Layer 3\n",
    "    fcnn = Conv2D(128, kernel_size=(3, 3), activation=relu_type)(fcnn)\n",
    "    fcnn = MaxPooling2D((2, 2), strides=(2, 2))(fcnn)\n",
    "    fcnn = Dropout(0.2)(fcnn)\n",
    "    \n",
    "    # Con Layer 4\n",
    "    fcnn = Conv2D(128, kernel_size=(3, 3), activation=relu_type)(fcnn)\n",
    "    fcnn = MaxPooling2D((2, 2), strides=(2, 2))(fcnn)\n",
    "    fcnn = Dropout(0.2)(fcnn)\n",
    "    fcnn = BatchNormalization()(fcnn)\n",
    "    \n",
    "    # 완전연결계층을 위해 평탄화\n",
    "    fcnn = Flatten()(fcnn)\n",
    "    \n",
    "    # 완전연결계층 전 모델 저장\n",
    "    # 결합 모델에서 사용 예정\n",
    "    local_input = input_1\n",
    "    partial_model = Model(input_1, fcnn)\n",
    "    \n",
    "    dense = Dropout(0.2)(fcnn)\n",
    "    # Dense Layer 1\n",
    "    dense = Dense(256, activation=relu_type)(dense)\n",
    "    dense = Dropout(0.2)(dense)\n",
    "    \n",
    "    # Dense Layer 2\n",
    "    dense = Dense(128, activation=relu_type)(dense)\n",
    "    dense = Dropout(0.2)(dense)\n",
    "    \n",
    "    # Dense Layer 3\n",
    "    dense = Dense(64, activation=relu_type)(dense)\n",
    "    dense = Dropout(0.2)(dense)\n",
    "    \n",
    "    # Sigmoid Layer\n",
    "    output = Dense(1, activation = 'sigmoid')(dense)\n",
    "    #############################################\n",
    "    \n",
    "    # 기본 모델저장\n",
    "    model = Model(local_input, output)\n",
    "    optimizer = Adam(lr=lr, decay=decay)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    \n",
    "    return model, partial_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-09-05T18:27:02.231Z"
    }
   },
   "outputs": [],
   "source": [
    "# 2개의 기본 모델로 학습하는 결합 모델 정의\n",
    "def combined_model(m_b, m_img, lr, decay):\n",
    "    input_b = Input(shape=(75, 75, 3))\n",
    "    input_img = Input(shape=(75, 75, 3))\n",
    "    \n",
    "    m1 = m_b(input_b)\n",
    "    m2 = m_img(input_img)\n",
    "    \n",
    "    # 2개의 모델의 결과를 입력으로 하여 최종결과 예측\n",
    "    ################# 모델 설계 #################\n",
    "    common = Concatenate()([m1, m2])\n",
    "    \n",
    "    # Batch Normalization Layer\n",
    "    common = BatchNormalization()(common)\n",
    "    common = Dropout(0.3)(common)\n",
    "    \n",
    "    # Dense Layer 1\n",
    "    common = Dense(1024, activation='relu')(common)\n",
    "    common = Dropout(0.3)(common)\n",
    "    \n",
    "    # Dense Layer 2\n",
    "    common = Dense(512, activation='relu')(common)\n",
    "    common = Dropout(0.3)(common)\n",
    "    \n",
    "    # Sigmoid Layer\n",
    "    output = Dense(1, activation='sigmoid')(common)\n",
    "    #############################################\n",
    "    \n",
    "    model = Model([input_b, input_img], output)\n",
    "    optimizer = Adam(lr=lr, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=decay)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-09-05T18:27:02.264Z"
    }
   },
   "outputs": [],
   "source": [
    "# Data Augmentation으로 batch_size만큼 데이터 생성\n",
    "def gen_flow_multi_inputs(I1, I2, y, batch_size):\n",
    "    # ImageDataGenerator 객체 생성\n",
    "    gen1 = ImageDataGenerator(horizontal_flip=True, vertical_flip=True,\n",
    "                             width_shift_range=0., height_shift_range=0.,\n",
    "                             channel_shift_range=0, zoom_range=0.2,\n",
    "                             rotation_range=10)\n",
    "    gen2 = ImageDataGenerator(horizontal_flip=True, vertical_flip=True,\n",
    "                             width_shift_range=0., height_shift_range=0.,\n",
    "                             channel_shift_range=0, zoom_range=0.2,\n",
    "                             rotation_range=10)\n",
    "    \n",
    "    # batch_size만큼의 Augmented Data 반환하는 Iterator 생성\n",
    "    # 해당 y와 I2를 반환하기 위해 I1을 일치시킴\n",
    "    genI1 = gen1.flow(I1, y, batch_size=batch_size, seed=57, shuffle=False)\n",
    "    genI2 = gen2.flow(I1, I2, batch_size=batch_size, seed=57, shuffle=False)\n",
    "    \n",
    "    while True:\n",
    "        I1i = genI1.next()\n",
    "        I2i = genI2.next()\n",
    "        \n",
    "        np.testing.assert_array_equal(I2i[0], I1i[0])\n",
    "        yield [I1i[0], I2i[1]], I1i[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-09-05T18:27:02.276Z"
    }
   },
   "outputs": [],
   "source": [
    "# 모델 학습\n",
    "def train_model(model, batch_size, epochs, checkpoint_name, X_train, y_train, val_data, verbose=2):\n",
    "    # 콜백 함수 정의\n",
    "    callbacks = [ModelCheckpoint(checkpoint_name, save_best_only=True, monitor='val_loss')]\n",
    "    # ImageDataGenerator 객체 생성\n",
    "    datagen = ImageDataGenerator(horizontal_flip=True, vertical_flip=True,\n",
    "                                width_shift_range=0., height_shift_range=0.,\n",
    "                                channel_shift_range=0, zoom_range=0.2,\n",
    "                                rotation_range=10)\n",
    "    \n",
    "    # 검증 데이터셋\n",
    "    X_test, y_test = val_data\n",
    "    \n",
    "    try:\n",
    "        # 모델 학습\n",
    "        model.fit_generator(datagen.flow(X_train, y_train, batch_size=batch_size),\n",
    "                           epochs=epochs, steps_per_epoch=len(X_train)/batch_size,\n",
    "                           validation_data=(X_test, y_test),\n",
    "                           verbose=1, callbacks=callbacks)\n",
    "    except KeyboardInterrupt:\n",
    "        if verbose > 0:\n",
    "            print('Interrupted')\n",
    "    if verbose > 0:\n",
    "        print('Loading model...')\n",
    "        \n",
    "        \n",
    "    # 최고 성능일 때 weight 값일 때의 모델\n",
    "    model.load_weights(checkpoint_name)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-09-05T18:27:02.285Z"
    }
   },
   "outputs": [],
   "source": [
    "# 기본 모델 학습\n",
    "def gen_model_weights(lr, decay, channels, relu, batch_size, epochs, path_name, data, verbose=2):\n",
    "    X_train, y_train, X_test, y_test, X_val, y_val = data\n",
    "    # 모델 객체 생성\n",
    "    model, partial_model = get_model_notebook(lr, decay, channels, relu)\n",
    "    # 모델 학습\n",
    "    model = train_model(model, batch_size, epochs, path_name,\n",
    "                       X_train, y_train, (X_test, y_test), verbose=verbose)\n",
    "    \n",
    "    if verbose > 0:\n",
    "        # 검증 데이터로 모델 성능 검증\n",
    "        loss_val, acc_val = model.evaluate(X_val, y_val,\n",
    "                                          verbose=0, batch_size=batch_size)\n",
    "        loss_train, acc_train = model.evaluate(X_test, y_test,\n",
    "                                              verbose=0, batch_size=batch_size)\n",
    "        print('val / train loss :', str(loss_val) + ' / ' + str(loss_train), \\\n",
    "             '- val / train acc :', str(acc_val) + ' / ' + str(acc_train))\n",
    "        \n",
    "    return model, partial_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-09-05T18:27:02.294Z"
    }
   },
   "outputs": [],
   "source": [
    "# 기본 모델 2개와 결합 모델 1개 모두 학습\n",
    "def train_models(dataset, lr, batch_size, max_epoch, verbose=2, return_model=False):\n",
    "    # 학습 데이터셋\n",
    "    y_train, X_b, X_images = dataset\n",
    "    \n",
    "    # 검증 데이터셋 추출\n",
    "    y_train_full, y_val, X_b_full, X_b_val, X_images_full, X_images_val = train_test_split(\n",
    "        y_train, X_b, X_images, random_state=687, train_size=0.9)\n",
    "    \n",
    "    # 학습 및 테스트 데이터 추출\n",
    "    y_train, y_test, X_b_train, X_b_test, X_images_train, X_images_test = train_test_split(\n",
    "        y_train_full, X_b_full, X_images_full, random_state=576, train_size=0.85)\n",
    "    \n",
    "    # 1. bandwidth 데이터로 학습\n",
    "    if train_b:\n",
    "        if verbose > 0:\n",
    "            print('Training bandwidth network...')\n",
    "        data_b1 = (X_b_train, y_train, X_b_test, y_test, X_b_val, y_val)\n",
    "        model_b, model_b_cut = gen_model_weights(lr, 1e-6, 3, 'relu', batch_size, max_epoch, 'model_b',\n",
    "                                                 data=data_b1, verbose=verbose)\n",
    "    \n",
    "    # 2. 이미지 데이터로 학습\n",
    "    if train_img:\n",
    "        if verbose > 0:\n",
    "            print('Training image network...')\n",
    "        data_images = (X_images_train, y_train, X_images_test, y_test, X_images_val, y_val)\n",
    "        model_images, model_images_cut = gen_model_weights(lr, 1e-6, 3, 'relu', batch_size, max_epoch, 'model_img',\n",
    "                                                       data_images, verbose=verbose)\n",
    "    \n",
    "    # 3. 결합 모델로 학습\n",
    "    if train_total:\n",
    "        common_model = combined_model(model_b_cut, model_images_cut, lr/2, 1e-7)\n",
    "        common_x_train = [X_b_full, X_images_full]\n",
    "        common_y_train = y_train_full\n",
    "        common_x_val = [X_b_val, X_images_val]\n",
    "        common_y_val = y_val\n",
    "        \n",
    "        if verbose > 0:\n",
    "            print('Training common network...')\n",
    "        callbacks = [ModelCheckpoint('common', save_best_only=True, monitor='val_loss')]\n",
    "        try:\n",
    "            common_model.fit_generator(gen_flow_multi_inputs(X_b_full, X_images_full, y_train_full, batch_size),\n",
    "                                      epochs=30, steps_per_epoch=len(X_b_full)/batch_size,\n",
    "                                      validation_data=(common_x_val, common_y_val), verbose=1,\n",
    "                                      callbacks=callbacks)\n",
    "        except KeyboardInterrupt:\n",
    "            pass\n",
    "        common_model.load_weights('common')\n",
    "        \n",
    "        # 학습/검증 데이터의 손실함수값과 정확도\n",
    "        loss_val, acc_val = common_model.evaluate(common_x_val, common_y_val,\n",
    "                                                 verbose=0, batch_size=batch_size)\n",
    "        loss_train, acc_train = common_model.evaluate(common_x_train, common_y_train,\n",
    "                                                     verbose=0, batch_size=batch_size)\n",
    "        \n",
    "        if verbose > 0:\n",
    "            print('loss :', loss_val, 'acc : ', acc_val)\n",
    "    \n",
    "    # 모델 반환 여부\n",
    "    if return_model:\n",
    "        return common_model\n",
    "    else:\n",
    "        return (loss_train, acc_train), (loss_val, acc_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-09-05T18:27:02.303Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 모델 학습\n",
    "# epoch: 250 / learning rate: 8e-5 / batch size: 32\n",
    "common_model = train_models((y_train, X_b, X_images),\n",
    "                           lr=7e-04, batch_size=32, max_epoch=50, \n",
    "                            verbose=1, return_model=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-09-05T18:27:02.319Z"
    }
   },
   "outputs": [],
   "source": [
    "# 최종 예측 submission 파일 생성\n",
    "if predict_submission:\n",
    "    print('Reading test dataset...')\n",
    "    test = pd.read_json('../data/test.json')\n",
    "    y_fin, X_fin_b, X_fin_img = create_dataset(test, False)\n",
    "    \n",
    "    print('Predicting...')\n",
    "    prediction = common_model.predict([X_fin_b, X_fin_img], verbose=1, batch_size=32)\n",
    "    \n",
    "    print('Submitting...')\n",
    "    submission = pd.DataFrame({'id': test['id'], 'is_iceberg': prediction.reshape((prediction.shape[0]))})\n",
    "    \n",
    "    submission.to_csv('../data/submission_3.csv', index=False)\n",
    "    print('Done')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
