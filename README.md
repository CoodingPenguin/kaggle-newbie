# 눈누난나 NunnuNanna 캐글 스터디

데이터분석 기술 스터디 그룹 눈누난나 NunnuNanna의 14주차 캐글 스터디 과제입니다.

캐글 코리아 블로그에 올라온 [이유한 님의 캐글 커널 커리큘럼](https://kaggle-kr.tistory.com/32)을 따라 데이터 분석 기술을 공부합니다.

## ✅ 체크표

|     | w1  | w2  | w3  | w4  | w5  | w6  | w7  | w8  | w9  | w10 | w11 | w12 | w13 | w14 |
| --- | :-: | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
| 1   |  ✔  |     |     |     |     |     |     |     |     |     |     |     |     |     |
| 2   |  ✔  |     |     |     |     |     |     |     |     |     |     |     |     |     |
| 3   |  ✔  |     |     |     |     |     |     |     |     |     |     |     |     |     |

## ✒ 공부 방법

1. 필사적으로 필사한다
2. 커널의 A부터 Z까지 다 똑같이 따라 적는다.
3. 똑같이 3번 적고 다음 커널로 넘어간다.

## 📄 커리큘럼

### Binary Classification: Tabular Data

### [1st level. Titanic: Machine Learning from Disaster](https://www.kaggle.com/c/titanic)

- [x] [타이타닉 튜토리얼 1 - exploratory data analysis, visualization, machine learning](https://kaggle-kr.tistory.com/17?category=868316)
- [ ] [eda to prediction(dietanic)](https://www.kaggle.com/ash316/eda-to-prediction-dietanic)
- [ ] [titanic top 4% with ensemble modeling](https://www.kaggle.com/yassineghouzam/titanic-top-4-with-ensemble-modeling)
- [ ] [introduction to ensembling/stacking in python](https://www.kaggle.com/arthurtok/introduction-to-ensembling-stacking-in-python)

### [2nd level. Porto Seguro’s Safe Driver Prediction](https://www.kaggle.com/c/porto-seguro-safe-driver-prediction)

- [ ] [data preparation & exploration](https://www.kaggle.com/bertcarremans/data-preparation-exploration)
- [ ] [interactive porto insights - a plot.ly tutorial](https://www.kaggle.com/arthurtok/interactive-porto-insights-a-plot-ly-tutorial)
- [ ] [xgboost cv (lb .284)](https://www.kaggle.com/aharless/xgboost-cv-lb-284)
- [ ] [porto seguro exploratory analysis and prediction](https://www.kaggle.com/gpreda/porto-seguro-exploratory-analysis-and-prediction)

### [3rd level. Home Credit Default Risk](https://www.kaggle.com/c/home-credit-default-risk)

- [ ] [introduction: home credit default risk competition](https://www.kaggle.com/willkoehrsen/start-here-a-gentle-introduction)
- [ ] [introduction to manual feature engineering](https://www.kaggle.com/willkoehrsen/introduction-to-manual-feature-engineering)
- [ ] [stacking test-sklearn, xgboost, catboost, lightgbm](https://www.kaggle.com/eliotbarr/stacking-test-sklearn-xgboost-catboost-lightgbm)
- [ ] [lightgbm 7th place solution](https://www.kaggle.com/jsaguiar/lightgbm-7th-place-solution)

### Multi-class Classification: Tabular Data

### [1st level. Costa-rican competition](https://www.kaggle.com/c/cost)

- [ ] [A Complete Introduction and Walkthrough](https://www.kaggle.com/willkoehrsen/a-complete-introduction-and-walkthrough)
- [ ] [3250feats->532 feats using shap[LB: 0.436]](https://www.kaggle.com/youhanlee/3250feats-532-feats-using-shap-lb-0-436)
- [ ] [XGBoost](https://www.kaggle.com/skooch/xgboost)

### Binary Classification: Image Classification

### [1st level. Statoil/C-CORE Iceberg Classifier Challenge](https://www.kaggle.com/c/statoil-iceberg-classifier-challenge)

- [ ] [keras model for beginners (0.210 on lb)+eda+r&d](https://www.kaggle.com/devm2024/keras-model-for-beginners-0-210-on-lb-eda-r-d)
- [ ] [transfer learning with vgg-16 cnn+aug lb 0.1712](https://www.kaggle.com/devm2024/transfer-learning-with-vgg-16-cnn-aug-lb-0-1712)
- [ ] [submarineering.even better public score until now.](https://www.kaggle.com/submarineering/submarineering-even-better-public-score-until-now)
- [ ] [keras+tf lb 0.18](https://www.kaggle.com/wvadim/keras-tf-lb-0-18)

### Multi-class Classification: Image Classification

### [1st level. Fruits 360](https://www.kaggle.com/uciml/mush)

- [ ] [Fruits-360 - Transfer Learning using Keras](https://www.kaggle.com/amadeus1996/fruits-360-transfer-learning-using-keras)

### [2nd level. Fruits 360](https://www.kaggle.com/zalando-research/fashionmnist)

- [ ] [How Autoencoders Work: Intro and UseCases](https://www.kaggle.com/shivamb/how-autoencoders-work-intro-and-usecases)
- [ ] [CNN with Keras](https://www.kaggle.com/bugraokcu/cnn-with-keras)

### Regression: Tabular data

### [1st level. New York City Taxi Trip Duration](https://www.kaggle.com/c/nyc-taxi-trip-duration)

- [ ] [dynamics of new york city - animation](https://www.kaggle.com/drgilermo/dynamics-of-new-york-city-animation)
- [ ] [eda + baseline model](https://www.kaggle.com/aiswaryaramachandran/eda-baseline-model-0-40-rmse)
- [ ] [beat the benchmark!](https://www.kaggle.com/danijelk/beat-the-benchmark)

### [2nd level. Zillow Prize: Zillow’s Home Value Prediction (Zestimate)](https://www.kaggle.com/c/zillow-prize-1)

- [ ] [simple exploration notebook - zillow prize](https://www.kaggle.com/sudalairajkumar/simple-exploration-notebook-zillow-prize)
- [ ] [simple xgboost starter (~0.0655)](https://www.kaggle.com/anokas/simple-xgboost-starter-0-0655)
- [ ] [zillow eda on missing values & multicollinearity](https://www.kaggle.com/viveksrinivasan/zillow-eda-on-missing-values-multicollinearity)
- [ ] [xgboost, lightgbm, and ols and nn](https://www.kaggle.com/aharless/xgboost-lightgbm-and-ols-and-nn)

### Object segmentation: Deep learning

### [1st level. 2018 Data Science Bowl](https://www.kaggle.com/c/data-science-bowl-2018)

- [ ] [teaching notebook for total imaging newbies](https://www.kaggle.com/stkbailey/teaching-notebook-for-total-imaging-newbies)
- [ ] [keras u-net starter - lb 0.277](https://www.kaggle.com/keegil/keras-u-net-starter-lb-0-277)
- [ ] [nuclei overview to submission](https://www.kaggle.com/kmader/nuclei-overview-to-submission)

### Natural language processing: classification, regression

### [1st level. Spooky Author Identification](https://www.kaggle.com/c/spooky-author-identification)

- [ ] [spooky nlp and topic modelling tutorial](https://www.kaggle.com/arthurtok/spooky-nlp-and-topic-modelling-tutorial)
- [ ] [approaching (almost) any nlp problem on kaggle](https://www.kaggle.com/abhishek/approaching-almost-any-nlp-problem-on-kaggle)
- [ ] [simple feature engg notebook - spooky author](https://www.kaggle.com/sudalairajkumar/simple-feature-engg-notebook-spooky-author)

### [2nd level. Mercari Price Suggestion Challenge](https://www.kaggle.com/c/mercari-price-suggestion-challenge)

- [ ] [mercari interactive eda + topic modelling](https://www.kaggle.com/thykhuely/mercari-interactive-eda-topic-modelling)
- [ ] [a simple nn solution with keras (~0.48611 pl)](https://www.kaggle.com/knowledgegrappler/a-simple-nn-solution-with-keras-0-48611-pl)
- [ ] [ridge (lb 0.41943)](https://www.kaggle.com/rumbok/ridge-lb-0-41944)
- [ ] [LGB and FM [18th Place - 0.40604]](https://www.kaggle.com/peterhurford/lgb-and-fm-18th-place-0-40604)

### [3rd level. Toxic Comment Classification Challenge](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge)

- [ ] [[For Beginners] Tackling Toxic Using Keras](https://www.kaggle.com/sbongo/for-beginners-tackling-toxic-using-keras)
- [ ] [stop the s@#\$ - toxic comments eda](https://www.kaggle.com/jagangupta/stop-the-s-toxic-comments-eda)
- [ ] [logistic regression with words and char n-grams](https://www.kaggle.com/tunguz/logistic-regression-with-words-and-char-n-grams)
- [ ] [classifying multi-label comments (0.9741 lb)](https://www.kaggle.com/rhodiumbeng/classifying-multi-label-comments-0-9741-lb)

### Other dataset : anomaly detection, visualization

### [1st level. Credit Card Fraud Detection](https://www.kaggle.com/mlg-ulb/creditcardfraud)

- [ ] [in depth skewed data classif. (93% recall acc now)](https://www.kaggle.com/joparga3/in-depth-skewed-data-classif-93-recall-acc-now)
- [ ] [anomaly detection - credit card fraud analysis](https://www.kaggle.com/pavansanagapati/anomaly-detection-credit-card-fraud-analysis)
- [ ] [semi-supervised anomaly detection survey](https://www.kaggle.com/matheusfacure/semi-supervised-anomaly-detection-survey)

### [2nd level. Kaggle Machine Learning & Data Science Survey 2017](https://www.kaggle.com/kaggle/kaggle-survey-2017)

- [ ] [novice to grandmaster](https://www.kaggle.com/ash316/novice-to-grandmaster)
- [ ] [what do kagglers say about data science ?](https://www.kaggle.com/mhajabri/what-do-kagglers-say-about-data-science)
- [ ] [plotly tutorial - 1](https://www.kaggle.com/hakkisimsek/plotly-tutorial-1)
